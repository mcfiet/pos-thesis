\section{Test Setup and Environment}

We provisioned two parallel Kubernetes clusters. One composed of Confidential VMs, the other of Non-Confidential VMs, across three geographic regions (us-central1, europe-north1, asia-east1). Each cluster consists of one master node (VM name suffix \texttt{-1}) and two worker nodes (\texttt{-2}, \texttt{-3}). At script startup, we detect the VM type (regular vs.\ confidential) by inspecting dmesg for AMD SEV memory-encryption features.

In total, we conducted 11 different benchmarks across both clusters. Out of these, 9 were considered representative and used for the comparative analysis. The remaining two were excluded due to inconsistent or non-reproducible behavior.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{img/benchmarks.png}
    \caption{Running 11 benchmarks (CPU utilization)}
    \label{fig:replica_cpu}
\end{figure}

Additionally, we included dedicated idle-time measurements during which no workloads were executed. These tests were designed to observe and compare the baseline behavior of the master nodes in both clusters, specifically to identify any systematic differences between Confidential and Non-Confidential VMs in the absence of active tasks.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{img/cluster_idle.png}
    \caption{Inspecting \enquote{idle} in cluster (CPU utilization)}
    \label{fig:replica_cpu}
\end{figure}

\subsection{Network Latency and Bandwidth Measurement}

Using \texttt{benchmark\_network.sh}, we first enumerate internal cluster peers via
\texttt{kubectl get nodes} and extract their InternalIP addresses. For each peer and for three public DNS targets (8.8.8.8, 1.1.1.1, 9.9.9.9), we issue three ICMP echo requests (\texttt{ping -c 3}), parse the average round-trip time, and log per-target latency. Next, we measure download throughput by fetching a 10 MiB test file (speedtest.ftp.otenet.gr/files/test10Mb.db) with \texttt{curl --max-time 20}, compute Mbps from transfer time, and report the result. All outputs are echoed to stdout, annotated with node name, VM type, and timestamp, and, if VHSM is available, written to Vault under \texttt{cubbyhole/benchmark-results}.

\subsection{CPU, Memory and File I/O Performance}

The \texttt{benchmark\_cpu.sh} script orchestrates four sysbench-based suites:
\begin{enumerate}
  \item \textbf{CPU Prime Calculation (60 s):} measures \texttt{events/sec} and latencies (avg, 95th, max).
  \item \textbf{Memory Throughput (3Ã—30 s):} sequential write, sequential read, and random access tests, capturing throughput (MiB/s) and average latency.
  \item \textbf{Context Switching:} collects context-switch rate and latency via \texttt{vmstat}, plus mutex operations/sec using \texttt{sysbench mutex}.
  \item \textbf{File I/O (30 s):} prepares a 1 GiB test file on the local OpenEBS-hostpath storage, then runs random read/write I/O, extracting IOPS, throughput, and avg latency.
\end{enumerate}
After execution, a comprehensive JSON document is generated locally (and vaulted), containing raw metrics and computed indices (e.g.\ CPU events per core, memory bandwidth ratio, context-switch efficiency).  

\subsection{CloudNativePG Benchmarking}

With \texttt{benchmark\_cnpg.sh}, we deploy a 3-instance PostgreSQL cluster via the CNPG operator, using \texttt{openebs-hostpath} (1 GiB per replica). Once all pods are Ready, we execute a series of \texttt{pgbench} workloads:
\begin{itemize}
  \item Standard mixed read/write (20 s)
  \item Read-only (20 s)
  \item High-concurrency: 25 clients (60 s), 50 clients (60 s)
  \item Sustained load: 10 clients (300 s)
  \item Custom write-heavy script (60 s)
\end{itemize}
Finally, we query table sizes and cache hit ratios from \texttt{pg\_stat\_database}, clean up temporary SQL files, and log all results with node and VM-type metadata.

Each of the three scripts follows a consistent pattern: environment detection, workload execution, metric parsing, structured output (JSON and console), and optional secure storage. This uniform approach enables direct comparison between Confidential and Non-Confidential VMs across network, CPU/memory, I/O, and database performance dimensions.


\section{Analysis and Discussion}

\subsection{CPU Utilization in idle: \texttt{pos-sec-1} vs. \texttt{pos-no-sec-1}}

The CPU utilization of two virtual machines, \texttt{pos-sec-1} (with security features enabled) and \texttt{pos-no-sec-1} (without security features), was analyzed under idle conditions. In this context, \enquote{idle} refers to the absence of any active application-level pods beyond the default system workloads. Both Kubernetes clusters remained operational, running only essential control-plane and infrastructure components such as \texttt{kube-system}, \texttt{openebs}, and internal networking services. This setup allowed us to observe potential differences in baseline resource consumption between Confidential and Non-Confidential VMs without interference from user-defined workloads.

The results show that \texttt{pos-sec-1} consistently consumed more CPU resources than \texttt{pos-no-sec-1}. Specifically, the average CPU utilization of \texttt{pos-sec-1} was approximately \textbf{27.7\% higher} than that of \texttt{pos-no-sec-1}. Similarly, the median utilization was around \textbf{27.5\% higher}. These differences indicate a notable impact of the enabled security mechanisms on baseline system performance.

\begin{longtable}{@{}lrrrrr@{}}
\caption{CPU Utilization Comparison Between \texttt{pos-sec-1} and \texttt{pos-no-sec-1}} \\
\toprule
\textbf{Metric} & \textbf{pos-sec-1} & \textbf{pos-no-sec-1} & \textbf{Absolute Diff.} & \textbf{Relative Diff. (\%)} \\ \midrule
\endfirsthead
\toprule
\textbf{Metric} & \textbf{pos-sec-1} & \textbf{pos-no-sec-1} & \textbf{Absolute Diff.} & \textbf{Relative Diff. (\%)} \\ \midrule
\endhead
Mean             & 0.1913   & 0.1498   & +0.0415   & 27.7\% \\
Median           & 0.1919   & 0.1505   & +0.0414   & 27.5\% \\
Standard Dev.    & 0.0072   & 0.0063   & +0.0009   & 14.1\% \\
Minimum          & 0.1539   & 0.1342   & +0.0197   & 14.7\% \\
Maximum          & 0.2157   & 0.1758   & +0.0399   & 22.7\% \\
\bottomrule
\end{longtable}

\subsection{Database Performance}

Our CloudNativePG PostgreSQL benchmarks revealed counter-intuitive performance characteristics between regular and confidential VMs. Table~\ref{tab:postgresql_performance} summarizes the key findings across different workload patterns.

\begin{table}[htbp]
\centering
\caption{PostgreSQL Performance Comparison (TPS = Transactions Per Second)}
\label{tab:postgresql_performance}
\begin{tabular}{lccc}
\toprule
\textbf{Workload Pattern} & \textbf{Regular VM} & \textbf{Confidential VM} & \textbf{Overhead} \\
\midrule
Single Client (1c, 20s) & 490.22 & 430.44 & -12.2\% \\
5 Clients (20s) & 1084.75 & 1090.53 & +0.5\% \\
10 Clients (20s) & 1185.22 & 1186.73 & +0.1\% \\
Read-Only (10c, 20s) & 11956.97 & 11886.31 & -0.6\% \\
High Concurrency (25c, 60s) & 1255.73 & 1232.97 & -1.8\% \\
High Concurrency (50c, 60s) & 1244.61 & 1213.04 & -2.5\% \\
Sustained Load (10c, 300s) & 1204.82 & 1210.99 & +0.5\% \\
\bottomrule
\end{tabular}
\end{table}

The results demonstrate a counter-intuitive performance pattern: while single-client workloads experienced a notable 12.2\% performance degradation, multi-client scenarios showed minimal overhead or even slight improvements. This suggests that AMD SEV-SNP's memory encryption mechanisms scale efficiently under concurrent load, with the encryption overhead becoming negligible as parallelism increases.

Most significantly, read-intensive workloads showed virtually no performance impact (-0.6\%), indicating that memory encryption does not substantially affect data retrieval operations. The sustained load test (300 seconds) actually showed a marginal improvement (+0.5\%) in confidential VMs, suggesting potential optimization benefits from the hardware-assisted memory management in AMD SEV-SNP.

\subsubsection{CPU Utilization of Primary vs. Replica Nodes}

During the benchmark execution, a clear distinction in CPU utilization between CloudNativePG \emph{primary} and \emph{replica} nodes could be observed. At specific points in time, the primary nodes (\texttt{pos-no-sec-2} and \texttt{pos-sec-3}) reached CPU utilizations close to or at 100\%. This indicates high processing activity, most likely due to write-heavy transactions and coordination overhead inherent to the primary role.

In contrast, the replica nodes exhibited considerably lower CPU load during these peak intervals. Their utilization ranged between approximately 22\% and 35\%, suggesting that their workload was largely shaped by streaming replication and read-only queries.

This behavior aligns well with the documented architecture of CloudNativePG, which assigns all write and transaction coordination duties to the primary instance, while replicas primarily serve read traffic and replicate the WAL (Write-Ahead Log) stream asynchronously from the primary\parencite{cloudnativepg_architecture}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{img/replica_peak_utilization.png}
    \caption{CPU utilization of replica nodes during primary CPU peaks (Main load on the primary pos-no-sec-2 and pos-sec-3 - other nodes less utilization)}
    \label{fig:replica_cpu}
\end{figure}

\subsection{CPU and Memory Performance}

Table~\ref{tab:cpu_memory_performance} presents the computational performance comparison between VM types.

\begin{table}[htbp]
\centering
\caption{CPU and Memory Performance Comparison}
\label{tab:cpu_memory_performance}
\begin{tabular}{lccc}
\toprule
\textbf{Metric} & \textbf{Regular VM} & \textbf{Confidential VM} & \textbf{Overhead} \\
\midrule
\multicolumn{4}{l}{\textit{CPU Performance}} \\
Prime Calculation (events/sec) & 1503.51 & 1475.80 & -1.8\% \\
CPU Intensive (events/sec) & 3912.74 & 3862.40 & -1.3\% \\
Average Latency (ms) & 1.330 & 1.355 & +1.9\% \\
\midrule
\multicolumn{4}{l}{\textit{Memory Performance}} \\
Sequential Write (MB/s) & 7577.11 & 7320.12 & -3.4\% \\
Sequential Read (MB/s) & 9880.25 & 9481.66 & -4.0\% \\
Random Access (MB/s) & 2551.84 & 2491.99 & -2.3\% \\
\bottomrule
\end{tabular}
\end{table}

The CPU performance measurements reveal exceptionally low overhead for confidential computing, with computational workloads experiencing less than 2\% performance degradation. This validates the efficiency of AMD SEV-SNP's hardware-assisted confidential computing implementation, where cryptographic operations are offloaded to dedicated silicon rather than consuming general-purpose CPU cycles.

Memory performance showed slightly higher but still acceptable overhead ranging from 2.3\% to 4.0\%. The sequential read operations experienced the highest impact at 4.0\%, which aligns with expectations given that AMD SEV-SNP encrypts all memory contents. Notably, the overhead remains consistent across different access patterns, indicating robust hardware implementation rather than software-layer encryption penalties.

\subsection{Storage and Network Performance}

File I/O performance demonstrated remarkable resilience under the dual encryption scheme employed (LUKS software encryption combined with AMD SEV hardware encryption). Both read and write IOPS experienced merely 1.0\% overhead, with I/O latency increasing by only 1.1\%. This validates the feasibility of layered encryption approaches in production environments.

Network performance analysis revealed interesting patterns. Intra-cluster communication between confidential VMs showed variable latency impacts: one inter-regional connection experienced a 7.1\% increase (125.18ms to 134.03ms), while another showed slight improvement (-0.9\%). Despite deploying VMs within the same regions and zones, differences in exact physical server locations within data centers may have contributed to these variations, as regions and zones can impact network latency \parencite{google_cloud_regions_zones_2024}. External connectivity to DNS servers demonstrated a 20.7\% latency increase, potentially attributable to additional cryptographic validation processes in the confidential computing stack.

Importantly, network bandwidth remained virtually unchanged (-0.6\%), confirming that the WireGuard VPN mesh combined with AMD SEV encryption does not create throughput bottlenecks for data-intensive applications.

\subsection{Statistical Significance and Consistency}

The statistical analysis reveals high consistency in confidential VM performance, with PostgreSQL transaction rates showing lower standard deviations (7-43 TPS) compared to regular VMs (42-86 TPS). This suggests that hardware-based confidential computing provides more predictable performance characteristics than software-only approaches.

CPU benchmarks maintained consistent sub-2\% overhead across all test iterations, with standard deviations remaining comparable between VM types. However, memory performance showed higher variability in confidential VMs, indicating potential optimization opportunities in memory management patterns.

\subsection{Production Readiness Assessment}

Based on our comprehensive evaluation, confidential VMs demonstrate production readiness for multi-tenant database workloads. The key findings supporting this assessment include:

\begin{enumerate}
\item \textbf{Acceptable Performance Overhead}: Less than 5\% impact across most critical workloads
\item \textbf{Scalability}: Improved performance characteristics under concurrent load
\item \textbf{Consistency}: More predictable performance patterns compared to regular VMs
\item \textbf{Storage Compatibility}: Seamless integration with encrypted storage solutions
\item \textbf{Network Resilience}: Minimal bandwidth impact despite layered encryption
\end{enumerate}

The single-client performance anomaly warrants further investigation but does not preclude production deployment, as most database workloads operate with multiple concurrent connections. The overall performance profile validates confidential computing as a viable solution for privacy-sensitive multi-cloud database deployments.
